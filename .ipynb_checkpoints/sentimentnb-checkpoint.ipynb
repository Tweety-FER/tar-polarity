{
 "metadata": {
  "name": "",
  "signature": "sha256:696b4848f4ef43603094dacf372766afe40d5367f12465cb03d19b616b2a0e14"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Aspect-Oriented Sentiment Analysis\n",
      "\n",
      "## Resources\n",
      "\n",
      "[Stemmer](http://nlp.ffzg.hr/resources/tools/stemmer-for-croatian/)\n",
      "\n",
      "[Lemmatiser](http://nlp.ffzg.hr/resources/models/tagging/)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Comment Structure Representation\n",
      "\n",
      "The comments in the dataset are representes as `XML`. To represent it, three classes are used - `TaggedDocument`, `TaggedSentence` and `TaggedWord`.\n",
      "\n",
      "---\n",
      "\n",
      "## `TaggedWord`\n",
      "The class represents a work with associated metadata - index in sentence, lemma, stem, POS tag and so on.\n",
      "\n",
      "## `TaggedSentence`\n",
      "A representation of a sentence with associated metadata. Each sentence has a list of `TaggedWord` objects representing the words that make it up.\n",
      "\n",
      "## `TaggedDocument`\n",
      "A representation of a document and its metadata, with a list of sentences that it contains as a list of `TaggedSentence` objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TaggedText(object):\n",
      "    \"\"\"Object representation of a review document tagged with sentence metadata, source and ratings.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.sentences = []\n",
      "        self.rating = 0\n",
      "        self.source = ''\n",
      "        self.text = ''\n",
      "\n",
      "    def add_sentence(self, sentence):\n",
      "        self.sentences.append(sentence)\n",
      "\n",
      "    def get_sentences(self):\n",
      "        return self.sentences\n",
      "        \n",
      "    def set_filename(self, fn):\n",
      "        self.filename = fn\n",
      "        \n",
      "    def get_filename(self):\n",
      "        return self.filename\n",
      "\n",
      "    def set_source_url(self, url):\n",
      "        self.source = url\n",
      "\n",
      "    def get_source_url(self):\n",
      "        return self.source\n",
      "\n",
      "    def set_rating(self, rating):\n",
      "        self.rating = rating\n",
      "\n",
      "    def get_rating(self):\n",
      "        return self.rating\n",
      "\n",
      "    def set_text(self, text):\n",
      "        self.text = text\n",
      "\n",
      "    def get_text(self):\n",
      "        return self.text\n",
      "    \n",
      "    def get_length(self):\n",
      "        return len(self.text)\n",
      "\n",
      "class TaggedSentence(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.words = []\n",
      "        self.dependencies = []\n",
      "        self.text = ''\n",
      "        self.start_position = 0\n",
      "        self.length = 0\n",
      "        self.annotated_pairs = []\n",
      "\n",
      "    def add_annotated_pair(self, aspect, anchor, link, sentiment):\n",
      "        r_link = transform_annotation(link)\n",
      "        r_sentiment = transform_annotation(sentiment)\n",
      "        self.annotated_pairs.append((aspect, anchor, r_link, r_sentiment))\n",
      "        \n",
      "    def get_annotated_pairs(self):\n",
      "        return self.annotated_pairs\n",
      "    \n",
      "    def get_annotated_pair(self, aspect, anchor):\n",
      "        aspect_word = aspect.get_word()\n",
      "        anchor_word = anchor.get_word()\n",
      "        \n",
      "        for (r_asp, r_anch, link, sentiment) in self.get_annotated_pairs():\n",
      "            if aspect_word == r_asp and anchor_word == r_anch:\n",
      "                return np.array([link, sentiment])\n",
      "            \n",
      "        return np.array([0, 0])\n",
      "        \n",
      "    def add_word(self, word):\n",
      "        self.words.append(word)\n",
      "\n",
      "    def get_words(self):\n",
      "        return self.words\n",
      "\n",
      "    def add_dependency(self, dependency):\n",
      "        self.dependencies.append(dependency)\n",
      "\n",
      "    def get_dependencies(self):\n",
      "        return self.dependencies\n",
      "\n",
      "    def set_text(self, text):\n",
      "        self.text = text\n",
      "\n",
      "    def get_text(self):\n",
      "        return self.text\n",
      "\n",
      "    def set_start_position(self, pos):\n",
      "        self.start_position = pos\n",
      "\n",
      "    def get_start_position(self):\n",
      "        return self.start_position\n",
      "\n",
      "    def get_length(self):\n",
      "        return len(self.text)\n",
      "    \n",
      "class Dependency(object):\n",
      "    def __init__(self, governor, dependent, relation):\n",
      "        self.governor = governor\n",
      "        self.dependent = dependent\n",
      "        self.relation = relation\n",
      "        \n",
      "    def get_governor_index(self):\n",
      "        return self.governor\n",
      "    \n",
      "    def get_dependent_index(self):\n",
      "        return self.dependent\n",
      "    \n",
      "    def get_relation(self):\n",
      "        return self.relation\n",
      "    \n",
      "    def __str__(self):\n",
      "        return '({0} as {1} to {2})'.format(self.governor, self.relation, self.dependent)\n",
      "\n",
      "\n",
      "class TaggedWord(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.word = ''\n",
      "        self.lemma = ''\n",
      "        self.molex_lemmas = []\n",
      "        self.POS = None\n",
      "        self.stem = ''\n",
      "        self.MSDs = []\n",
      "        self.position = 0\n",
      "        self.index = 0\n",
      "\n",
      "    def set_word(self, word):\n",
      "        self.word = word\n",
      "\n",
      "    def get_word(self):\n",
      "        return self.word\n",
      "\n",
      "    def set_stem(self, stem):\n",
      "        self.stem = stem\n",
      "\n",
      "    def get_stem(self):\n",
      "        return self.stem\n",
      "\n",
      "    def set_lemma(self, lemma):\n",
      "        self.lemma = lemma\n",
      "\n",
      "    def get_lemma(self):\n",
      "        return self.lemma\n",
      "\n",
      "    def add_molex_lemma(self, m_lemma):\n",
      "        self.molex_lemmas.append(m_lemma)\n",
      "\n",
      "    def get_molex_lemmas(self):\n",
      "        return self.molex_lemmas\n",
      "\n",
      "    def set_POS_tag(self, POS):\n",
      "        self.POS = POS\n",
      "\n",
      "    def get_POS_tag(self):\n",
      "        return self.POS\n",
      "\n",
      "    def add_MSD(self, msd):\n",
      "        self.MSDs.append(msd)\n",
      "\n",
      "    def get_MSDs(self):\n",
      "        return self.MSDs\n",
      "\n",
      "    def set_position(self, pos):\n",
      "        self.position = pos\n",
      "\n",
      "    def get_position(self):\n",
      "        return self.position\n",
      "\n",
      "    def set_index(self, index):\n",
      "        self.index = index\n",
      "\n",
      "    def get_index(self):\n",
      "        return self.index\n",
      "    \n",
      "def transform_annotation(annotation):\n",
      "    \"\"\"Transforms string-based orientation annotations into numbers\"\"\"\n",
      "    #if annotation == '+':\n",
      "    #    return 1\n",
      "    #if annotation == '-':\n",
      "    #    return -1\n",
      "    if annotation in \"+-\":\n",
      "        return 1\n",
      "    return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Parsing documents\n",
      "\n",
      "There must be a parsing process that turns the `XML` into the document representation provided above. The function `read_document` does the job for a single document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "import re\n",
      "\n",
      "def files_in(directory):\n",
      "    \"\"\"\n",
      "    List all files in directory. Excludes directories.\n",
      "    Args:\n",
      "        directory (str): String representation of path\n",
      "    Returns:\n",
      "        str[]: List of file names, not prefixed with directory name\n",
      "    \"\"\"\n",
      "    return [f for f in listdir(directory) if isfile(join(directory,f))]\n",
      "\n",
      "def read_documents_in(directory):\n",
      "    \"\"\"\n",
      "    Parses all XML documents in a given directory to create a list of object\n",
      "    document representations.\n",
      "    \n",
      "    Args:\n",
      "        directory (str) : The directory path\n",
      "    \n",
      "    Returns:\n",
      "        TaggedDocument[] : List of documents as object\n",
      "    \"\"\"\n",
      "    files = files_in(directory)\n",
      "    documents = []\n",
      "    \n",
      "    for f in files:\n",
      "        documents.append(read_document(join(directory, f)))\n",
      "        \n",
      "    return documents\n",
      "\n",
      "def read_document(doc):\n",
      "    \"\"\"\n",
      "    Parses an XML document to returns its tagged document representation\n",
      "    Args:\n",
      "        doc (str): The path to the document\n",
      "    Returns:\n",
      "        TaggedDocument: Parsed XML as object\n",
      "    \"\"\"\n",
      "    tree = ET.parse(doc)\n",
      "    root = tree.getroot()\n",
      "    \n",
      "    filename = doc.split('/')[-1].split('.')[0]\n",
      "    \n",
      "    comment = TaggedText()\n",
      "    comment.set_filename(filename) # Name the document\n",
      "    \n",
      "    text = root.find('Text').text\n",
      "    rating = float(root.find('Rating').text)\n",
      "    source = root.find('Source').text\n",
      "\n",
      "    # Setting the basic properties\n",
      "    comment.set_text(text)\n",
      "    comment.set_rating(rating)\n",
      "    comment.set_source_url(source)\n",
      "\n",
      "    # Iterating through all sentences in the text\n",
      "    sentences = root.find('Sentences')\n",
      "\n",
      "    for sentence in sentences.findall('SentenceInfo'):\n",
      "        tagged_sentence = TaggedSentence()\n",
      "\n",
      "        text = sentence.find('Text').text\n",
      "        start = int(sentence.find('StartPosition').text)\n",
      "\n",
      "        tagged_sentence.set_text(text)\n",
      "        tagged_sentence.set_start_position(start)\n",
      "\n",
      "        words = sentence.find('TaggedWords')\n",
      "\n",
      "        for word in words.findall('CROTaggedWord'):\n",
      "            tagged_word = TaggedWord()\n",
      "\n",
      "            word_text = word.find('Word').text\n",
      "            word_lemma = word.find('Lemma').text\n",
      "            molex_lemmas = [molex.text for molex in word.find('MolexLemmas').findall('string')]\n",
      "            POS = word.find('POSTag').text\n",
      "            stem = word.find('BasicStem').text\n",
      "            MSDs = [ msd.text for msd in word.find('MSDs').findall('string')]\n",
      "            position = int(word.find('Position').text)\n",
      "            index = int(word.find('SentenceIndex').text)\n",
      "\n",
      "            tagged_word.set_word(word_text)\n",
      "            tagged_word.set_lemma(word_lemma)\n",
      "            for molex in molex_lemmas: tagged_word.add_molex_lemma(molex)\n",
      "            tagged_word.set_stem(stem)\n",
      "            for msd in MSDs: tagged_word.add_MSD(msd)\n",
      "            tagged_word.set_position(position)\n",
      "            tagged_word.set_index(index)\n",
      "            tagged_word.set_POS_tag(POS)\n",
      "\n",
      "            tagged_sentence.add_word(tagged_word)\n",
      "            \n",
      "        dependencies = sentence.find('DependencyRelations')\n",
      "        \n",
      "        for dependency in dependencies.findall('DependencyRelation'):\n",
      "            governor = int(dependency.find('Governor').find('SentenceIndex').text)\n",
      "            dependent = int(dependency.find('Dependent').find('SentenceIndex').text)\n",
      "            relation = dependency.find('Relation').text\n",
      "            tagged_sentence.add_dependency(Dependency(governor, dependent, relation))\n",
      "            \n",
      "\n",
      "        comment.add_sentence(tagged_sentence)\n",
      "\n",
      "    return comment\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Aspects and Clues\n",
      "\n",
      "For processing to be done, we need to read in the aspects, as well as the positive and negative clues.\n",
      "\n",
      "## tf-idf\n",
      "\n",
      "For viable processing, tf-idf calculations need to be defined"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "import codecs\n",
      "import json\n",
      "\n",
      "def read_word_roots(document):\n",
      "    \"\"\"\n",
      "        Read stems from a file formatted as\n",
      "        >>>word\\tstem\n",
      "        This format is used for stemmer output\n",
      "    \"\"\"\n",
      "    stems = []\n",
      "    with codecs.open(document, 'r', 'utf-8') as fp:\n",
      "        for line in fp:\n",
      "            stems.append(line.split()[1]) # Stems/lemmas are in the second tab-delimited column\n",
      "    \n",
      "    return set(stems)\n",
      "\n",
      "lemma_aspects = read_word_roots('CropinionDataset/dictionary_lemmatised/aspects.txt')\n",
      "lemma_pos_clues = read_word_roots('CropinionDataset/dictionary_lemmatised/positive_clues.txt')\n",
      "lemma_neg_clues = read_word_roots('CropinionDataset/dictionary_lemmatised/negative_clues.txt')\n",
      "\n",
      "stem_aspects = read_word_roots('CropinionDataset/dictionary_stemmed/aspects.txt')\n",
      "stem_pos_clues = read_word_roots('CropinionDataset/dictionary_stemmed/positive_clues.txt')\n",
      "stem_neg_clues = read_word_roots('CropinionDataset/dictionary_stemmed/negative_clues.txt')\n",
      "\n",
      "aspects = lemma_aspects | stem_aspects\n",
      "pos_clues = lemma_pos_clues | stem_pos_clues\n",
      "neg_clues = lemma_neg_clues | stem_neg_clues \n",
      "\n",
      "all_clues = pos_clues | neg_clues\n",
      "\n",
      "# We use only aspects and clues as words (at this point)\n",
      "all_words = aspects | all_clues\n",
      "\n",
      "def get_word_list(docs, processor=TaggedWord.get_lemma):\n",
      "    word_list = []\n",
      "    \n",
      "    # Record occurrences\n",
      "    for doc in docs:\n",
      "        doc_name = doc.get_filename()\n",
      "        \n",
      "        for sentence in doc.get_sentences():\n",
      "            for word in sentence.get_words():\n",
      "                word_list.append(processor(word))\n",
      "    \n",
      "    return set(word_list)\n",
      "    \n",
      "\n",
      "def get_idf(docs, processor=TaggedWord.get_lemma, word_list=None):\n",
      "    N = len(docs) # Document count\n",
      "    idfs = {}\n",
      "    \n",
      "    # Record occurrences\n",
      "    for doc in docs:\n",
      "        doc_name = doc.get_filename()\n",
      "        \n",
      "        for sentence in doc.get_sentences():\n",
      "            for word in sentence.get_words():\n",
      "                root = processor(word)\n",
      "                if word_list != None and root not in word_list:\n",
      "                    continue # Skip unfamiliar words\n",
      "                if root in idfs:\n",
      "                    idfs[root][doc_name] = 1\n",
      "                else:\n",
      "                    idfs[root] = { doc_name : 1 }\n",
      "    \n",
      "    # Actual idfs:\n",
      "    for root in idfs:\n",
      "        root_doc_count = len(idfs[root]) + 1.0 # Smoothing\n",
      "        idfs[root] = log(N / root_doc_count)\n",
      "    \n",
      "    return idfs\n",
      "\n",
      "def get_tf(docs, processor=TaggedWord.get_lemma, word_list=set()):\n",
      "    tfs = {}\n",
      "    \n",
      "    # Record occurrences\n",
      "    for doc in docs:\n",
      "        doc_name = doc.get_filename()\n",
      "        tfs[doc_name] = {}\n",
      "        \n",
      "        for sentence in doc.get_sentences():\n",
      "            for word in sentence.get_words():\n",
      "                root = processor(word)\n",
      "                if word_list != None and root not in word_list:\n",
      "                    continue # Skip unfamiliar words\n",
      "                    \n",
      "                if root in tfs:\n",
      "                    tfs[doc_name][root] += 1\n",
      "                else:\n",
      "                    tfs[doc_name][root] = 1\n",
      "    \n",
      "    for doc_name in tfs:\n",
      "        # Skip documents without any recognised lemmas\n",
      "        if not tfs[doc_name].keys():\n",
      "            continue\n",
      "        max_freq = max(tfs[doc_name].values())\n",
      "        \n",
      "        for root in tfs[doc_name]:\n",
      "            tfs[doc_name][root] = 0.5 + 0.5 * tfs[doc_name][root] / max_freq\n",
      " \n",
      "    return tfs\n",
      "\n",
      "def tf_idf_vector(document_name, tfs, idfs, word_list):\n",
      "    tf_idf = []\n",
      "    \n",
      "    for word in word_list:\n",
      "        if document_name in tfs and word in tfs[document_name] and word in idfs:\n",
      "            tf_idf.append(tfs[document_name][word] * idfs[word])\n",
      "        else:\n",
      "            tf_idf.append(0.0)\n",
      "        \n",
      "    return np.array(tf_idf)\n",
      "\n",
      "train_docs = read_documents_in('CropinionDataset/reviews_new/train2/')\n",
      "test_docs = read_documents_in('CropinionDataset/reviews_new/test2/')\n",
      "\n",
      "# We must use train idfs in both cases to prevent information leakage\n",
      "word_list = get_word_list(train_docs) # all_words\n",
      "all_idfs = get_idf(train_docs, word_list=word_list)\n",
      "test_tfs = get_tf(train_docs, word_list=word_list)\n",
      "train_tfs = get_tf(test_docs, word_list=word_list)\n",
      "\n",
      "all_tfs = dict(test_tfs, **train_tfs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finding potential pairs\n",
      "\n",
      "We need to go through the text and find all potential aspect-clue pairs. Then we tag the bastards."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import remove\n",
      "from os.path import exists, join\n",
      "\n",
      "def get_all_lemmas(word):\n",
      "    \"\"\"Locates all lemmas and all molex lemmas\"\"\"\n",
      "    return word.get_molex_lemmas() + [word.get_lemma()]\n",
      "\n",
      "def get_all_roots(word):\n",
      "    \"\"\"Locates all lemmas, molex lemmas and stems\"\"\"\n",
      "    return word.get_molex_lemmas() + [word.get_lemma(), word.get_stem()]\n",
      "\n",
      "def find_pairs(sentence, aspects, clues, processor=get_all_roots):\n",
      "    \"\"\"\n",
      "    Finds all potential clue-aspect pairs in the sentence. Pairs are identified by comparing word stems\n",
      "    in the sentence to stems given in the aspect and clue dictionaries (sets).\n",
      "    \n",
      "    Args:\n",
      "        sentence (TaggedSentence) : Object representation of a sentence\n",
      "        aspects (set(str)) : Set of aspect stems\n",
      "        clues (set(str)) : Set of clue stems\n",
      "        \n",
      "    Returns:\n",
      "        ([(TaggedWord, TaggedWord)]) : List of TaggedWord candidate pairs\n",
      "    \"\"\"\n",
      "    sent_aspects = []\n",
      "    sent_clues = []\n",
      "        \n",
      "    for word in sentence.get_words():\n",
      "        root_word = processor(word)\n",
      "        if isinstance(root_word, list):\n",
      "            if any([w in aspects for w in root_word]):\n",
      "                sent_aspects.append(word)\n",
      "            if any([w in clues for w in root_word]):\n",
      "                sent_clues.append(word)\n",
      "        else:\n",
      "            if root_word in aspects:\n",
      "                sent_aspects.append(word)\n",
      "            if root_word in clues:\n",
      "                sent_clues.append(word)\n",
      "        \n",
      "    return [(aspect, clue) for aspect in sent_aspects for clue in sent_clues]\n",
      "\n",
      "def get_annotated_pairs_in_document(document):\n",
      "    tree = ET.parse(document)\n",
      "    root = tree.getroot()\n",
      "\n",
      "    pairs = []\n",
      "    for sentence in root.iter('sentence'):\n",
      "        sent_pairs = []\n",
      "        \n",
      "        for pair in sentence.iter('pair'):\n",
      "            if pair.get('link') in \"+-\":\n",
      "                sent_pairs.append((pair.get('aspect'), pair.get('anchor')))\n",
      "        pairs.append(sent_pairs)\n",
      "            \n",
      "    return pairs\n",
      "\n",
      "def get_unlocated_sentence_pair_count(found_pairs, real_pairs):\n",
      "    unlocated = 0\n",
      "    \n",
      "    for (aspect, clue) in real_pairs:\n",
      "        found = False\n",
      "        for (w_a, w_c) in found_pairs:\n",
      "            w_a, w_c = w_a.get_word(), w_c.get_word()\n",
      "            if (w_a == aspect and w_c == clue) or (w_c == aspect and w_a == clue) :\n",
      "                found = True\n",
      "                break\n",
      "        \n",
      "        if not found:\n",
      "            unlocated += 1\n",
      "            \n",
      "    return unlocated\n",
      "\n",
      "def get_unlocated_document_pair_count(document, ann_document, aspects, clues):\n",
      "    try:\n",
      "        ann_pairs = get_annotated_pairs_in_document(ann_document)\n",
      "    except:\n",
      "        #print ann_document, 'does not exist'\n",
      "        return 0 #No file, missed nothing\n",
      "                \n",
      "    parsed_doc = read_document(document)\n",
      "    \n",
      "    count = 0\n",
      "    \n",
      "    N = len(ann_pairs)\n",
      "    \n",
      "    for i in xrange(N):\n",
      "        my_pairs = find_pairs(parsed_doc.get_sentences()[i], aspects, clues)\n",
      "        ap_pairs = ann_pairs[i]\n",
      "        count += get_unlocated_sentence_pair_count(my_pairs, ap_pairs)\n",
      "        \n",
      "    return count\n",
      " \n",
      "def get_dataset_unlocated_pair_count(dataset_dir, annotated_pairs_dir, aspects, clues):\n",
      "    count = 0\n",
      "    files = files_in(dataset_dir)\n",
      "    \n",
      "    for doc in files:\n",
      "        my_doc = join(dataset_dir, doc)\n",
      "        ap_doc = join(annotated_pairs_dir, doc)\n",
      "        \n",
      "        count += get_unlocated_document_pair_count(my_doc, ap_doc, aspects, clues)\n",
      "        \n",
      "    return count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Extracting actual pairs and their annotations\n",
      "\n",
      "The features are listed and their annotations provided. We must record them in a separate file, for great justice and ease of use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "from os import remove\n",
      "from os.path import exists  \n",
      "\n",
      "def doc_load_annotated_pairs(document, src_dir):\n",
      "    \"\"\"\n",
      "    Given a TaggedDocument representation of an xml document, loads the manually annotated pairs and their\n",
      "    sentiments from a provided source directory. The pair are stored on the document - more precisely, on\n",
      "    each TaggedSentence within the document.\n",
      "    \"\"\"\n",
      "    filename = join(src_dir, document.get_filename() + '.xml')\n",
      "    \n",
      "    if not os.path.isfile(filename):\n",
      "        return False # No pairs\n",
      "        \n",
      "    doc_sentences = document.get_sentences()\n",
      "    \n",
      "    ap_tree = ET.parse(filename)\n",
      "    ap_root = ap_tree.getroot()\n",
      "    \n",
      "    ap_sentences = list(ap_root.iter('sentence'))\n",
      "    \n",
      "    sent_count = len(ap_sentences)\n",
      "    \n",
      "    for i in xrange(sent_count):\n",
      "        ap_sent = ap_sentences[i]\n",
      "        doc_sent = doc_sentences[i]\n",
      "        \n",
      "        # Find all pairs and append them to the doc\n",
      "        for pair in ap_sent.iter('pair'):\n",
      "            doc_sent.add_annotated_pair(pair.get('aspect'), pair.get('anchor'), pair.get('link'), pair.get('sent'))\n",
      "    \n",
      "    return True\n",
      "    \n",
      "\n",
      "def doc_annotate_located_pairs(document, aspects, clues):\n",
      "    marks = []\n",
      "    \n",
      "    for sentence in document.get_sentences():\n",
      "        pairs = find_pairs(sentence, aspects, clues)\n",
      "        for pair in pairs:\n",
      "            marks.append(sentence.get_annotated_pair(pair[0], pair[1]))\n",
      "            \n",
      "    \n",
      "    return np.array(marks)\n",
      "\n",
      "def annotate_set_pairs(src_dir, dest_file, annotations_dir, aspects, clues):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    status = False\n",
      "    \n",
      "    for doc in docs:\n",
      "        document = read_document(join(src_dir, doc))\n",
      "        doc_load_annotated_pairs(document, annotations_dir)\n",
      "        np.savetxt(fp, doc_annotate_located_pairs(document, aspects, clues))\n",
      "        \n",
      "    fp.close()\n",
      "    \n",
      "\n",
      "def annotate_regression(src_dir, dest_file):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    status = False\n",
      "    scores = []\n",
      "    for doc in docs:\n",
      "        document = read_document(join(src_dir, doc))\n",
      "        doc_score = document.get_rating()\n",
      "        \n",
      "        positive = 1 if doc_score >= 4 else 0\n",
      "        \n",
      "        score = np.array([doc_score, positive])\n",
      "        scores.append(score)\n",
      "    \n",
      "    np.savetxt(fp, np.array(scores))\n",
      "        \n",
      "    fp.close()\n",
      "\n",
      "def annotate_sets():\n",
      "    if(exists('features/train/y.txt')): remove('features/train/y.txt')\n",
      "    if(exists('features/test/y.txt')): remove('features/test/y.txt')  \n",
      "        \n",
      "    if(exists('features/train/z.txt')): remove('features/train/z.txt')\n",
      "    if(exists('features/test/z.txt')): remove('features/test/z.txt')  \n",
      "    \n",
      "    \n",
      "    annotate_set_pairs('CropinionDataset/reviews_new/train2', 'features/train/y.txt', 'CropinionDataset/annotated_pairs/all', aspects, all_clues)\n",
      "    annotate_set_pairs('CropinionDataset/reviews_new/test2', 'features/test/y.txt', 'CropinionDataset/annotated_pairs/all', aspects, all_clues)\n",
      "    \n",
      "    annotate_regression('CropinionDataset/reviews_new/train2', 'features/train/z.txt')\n",
      "    annotate_regression('CropinionDataset/reviews_new/test2', 'features/test/z.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feature extraction\n",
      "\n",
      "Here goes nothing!\n",
      "\n",
      "## Classification features\n",
      "\n",
      "The features used in the initial classification (finding pairs) runs are:\n",
      "\n",
      "- Absolute difference of pair word indices in sentence\n",
      "- Absolute difference of pair word beginnings in string\n",
      "- The length of the sentence\n",
      "- Whether a dependency relation exists between them\n",
      "- Whether their number matches\n",
      "- Whether their gender matches\n",
      "- Whether their cases match\n",
      "- Their POS tags as one-hot vectors\n",
      "- The number of positive and negative clues in the sentence\n",
      "- Whether there is negation in a 3-wide window from **each** word\n",
      "\n",
      "## Regression features\n",
      "\n",
      "The features (to be) used in the initial regression run:\n",
      "\n",
      "- tf-idf (still somewhat broken)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_set_features(src_dir, dest_file, aspects, pos_clues, neg_clues):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    \n",
      "    for doc in docs:\n",
      "        features = extract_document_features(join(src_dir, doc), aspects, pos_clues, neg_clues)\n",
      "        \n",
      "        np.savetxt(fp, features)\n",
      "    \n",
      "    fp.close()\n",
      "\n",
      "def extract_document_features(document_path, aspects, pos_clues, neg_clues):\n",
      "    document = read_document(document_path)\n",
      "    clues = pos_clues | neg_clues\n",
      "    \n",
      "    features = []\n",
      "    for sentence in document.get_sentences():\n",
      "        pairs = find_pairs(sentence, aspects, clues)\n",
      "        for pair in pairs:\n",
      "            features.append(compute_feature_vector(pair, sentence, pos_clues, neg_clues))\n",
      "            \n",
      "    return np.array(features)\n",
      "\n",
      "def compute_feature_vector(pair, sentence, pos_clues, neg_clues):\n",
      "    features = []\n",
      "    \n",
      "    features.append(pair_distance_index(pair))\n",
      "    features.append(pair_init_distances(pair))\n",
      "    features.append(sentence_length(sentence))\n",
      "    features.append(govern_relations_exist(pair, sentence))\n",
      "    features.append(match(pair, extract_plurality))\n",
      "    features.append(match(pair, extract_genders))\n",
      "    features.append(POS_vector(pair[0]))\n",
      "    features.append(POS_vector(pair[1]))\n",
      "    features.append(aspect_counts(sentence, pos_clues, neg_clues))\n",
      "    features.append(negation_present(pair[0], sentence))\n",
      "    features.append(negation_present(pair[1], sentence))\n",
      "    \n",
      "    return np.hstack(features)\n",
      "\n",
      "\n",
      "\n",
      "def aspect_counts(sentence, pos_clues, neg_clues, processor = get_all_lemmas):\n",
      "    counts = [0, 0]\n",
      "    for word in sentence.get_words():\n",
      "        root = processor(word)\n",
      "        if isinstance(root, list):\n",
      "            if any([r in pos_clues for r in root]):\n",
      "                counts[0] += 1\n",
      "            if any([r in neg_clues for r in root]):\n",
      "                counts[1] += 1\n",
      "        else:\n",
      "            if root in pos_clues:\n",
      "                counts[0] += 1\n",
      "            if root in neg_clues:\n",
      "                counts[1] += 1\n",
      "    \n",
      "    return np.array(counts)\n",
      "        \n",
      "def tf_idf_between(pair, sentence):\n",
      "    i_from = pair[0].get_index()\n",
      "    i_to = pair[1].get_index()\n",
      "    \n",
      "    if i_from > t_to:\n",
      "        i_from, i_to = i_to, i_from\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "def pair_distance_index(pair):\n",
      "    return np.array([abs(pair[0].get_index() - pair[1].get_index())])\n",
      "\n",
      "def pair_init_distances(pair):\n",
      "    return np.array([abs(pair[0].get_position() - pair[1].get_position())])\n",
      "\n",
      "def sentence_length(sentence):\n",
      "    return np.array([sentence.get_length()])\n",
      "\n",
      "def govern_relations_exist(pair, sentence):\n",
      "    rels = [0, 0] # Aspect governs clue, clue governs aspect\n",
      "    index_pair = pair[0].get_index(), pair[1].get_index()\n",
      "    \n",
      "    for dependency in sentence.get_dependencies():\n",
      "        (a, b) = dependency.get_governor_index(), dependency.get_dependent_index()\n",
      "        if (a,b) == index_pair:\n",
      "            rels[0] = 1\n",
      "        elif (b, a) == index_pair:\n",
      "            rels[1] = 1\n",
      "    \n",
      "    return np.array(rels)\n",
      "\n",
      "def match(pair, f):\n",
      "    options_a = f(pair[0])\n",
      "    options_b = f(pair[1])\n",
      "    \n",
      "    for a_g in options_a:\n",
      "        for b_g in options_b:\n",
      "            if a_g == b_g:\n",
      "                return np.array([1.0])\n",
      "    return np.array([0.0])\n",
      "\n",
      "def POS_vector(word):\n",
      "    tags = \"ACIMNPQRSVYZ\" # POS tags in set\n",
      "    one_hot = np.zeros(12)\n",
      "    \n",
      "    #print tags, word.get_POS_tag()\n",
      "    index = tags.index(word.get_POS_tag())\n",
      "    \n",
      "    if index != -1:\n",
      "        one_hot[index] = 1\n",
      "        \n",
      "    return one_hot\n",
      "\n",
      "def extract_plurality(word):\n",
      "    pluralities = set()\n",
      "    \n",
      "    msds = word.get_MSDs()\n",
      "    for msd in msds:\n",
      "        if msd[0] == 'A':\n",
      "            pluralities.add(msd[4])\n",
      "        elif msd[0] == 'N':\n",
      "            pluralities.add(msd[3])\n",
      "        elif msd[0] == 'V':\n",
      "            pluralities.add(msds[-1])\n",
      "            \n",
      "    return list(pluralities)\n",
      "\n",
      "def extract_genders(word):\n",
      "    genders = set()\n",
      "    \n",
      "    msds = word.get_MSDs()\n",
      "    for msd in msds:\n",
      "        if msd[0] == 'A':\n",
      "            genders.add(msd[3])\n",
      "        elif msd[0] == 'N':\n",
      "            genders.add(msd[2])\n",
      "            \n",
      "    return list(genders)\n",
      "\n",
      "def negation_present(word, sentence, k = 3):\n",
      "    negations = [u'ne', u'nije', u'nimalo', u'nipo\u0161to', u'nisam', u'nisu', u'nismo', u'nemojte', u'nikako', u'ne\u0107e']\n",
      "    word_index = word.get_index()\n",
      "    negation_present = 0\n",
      "    \n",
      "    words = sentence.get_words()\n",
      "    N = len(words)\n",
      "    \n",
      "    start = max(0, word_index - k)\n",
      "    end   = min(N - 1, word_index + k)\n",
      "    \n",
      "    while start <= end:\n",
      "        if start == word_index:\n",
      "            start += 1\n",
      "            continue\n",
      "        \n",
      "        if words[start].get_word().lower() in negations:\n",
      "            negation_present = 1\n",
      "            break # Just looking for presence, not count\n",
      "            \n",
      "        start += 1\n",
      "    \n",
      "    return np.array([negation_present])\n",
      "\n",
      "def get_BOW(document):\n",
      "    return tf_idf_vector(document.get_filename(), all_tfs, all_idfs, word_list)\n",
      "\n",
      "def extract_regression_features(src_dir, dest_file):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    \n",
      "    all_features = []\n",
      "\n",
      "    \n",
      "    for doc in docs:\n",
      "        document = read_document(join(src_dir, doc))\n",
      "        features = get_BOW(document)\n",
      "        length = np.array([document.get_length()])\n",
      "        \n",
      "        features = np.hstack([features, length])\n",
      "        all_features.append(features)\n",
      "        \n",
      "    np.savetxt(fp, np.array(all_features))\n",
      "    \n",
      "    fp.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import remove\n",
      "from os.path import exists\n",
      "\n",
      "def store_feature_vectors():\n",
      "    \"\"\"\n",
      "    Calculate the feature vectors for the entire train and test sets and store the resulting numpy vectors\n",
      "    into files for easy access. Should be run every time the way features are calculated has changed.\n",
      "    \"\"\"\n",
      "    #Remove old ones\n",
      "    if(exists('features/train/classification.txt')): remove('features/train/classification.txt')\n",
      "    if(exists('features/test/classification.txt')): remove('features/test/classification.txt')\n",
      "        \n",
      "    if(exists('features/train/regression.txt')): remove('features/train/regression.txt')\n",
      "    if(exists('features/test/regression.txt')): remove('features/test/regression.txt')\n",
      "        \n",
      "    extract_set_features('CropinionDataset/reviews_new/train2', 'features/train/classification.txt', aspects, pos_clues, neg_clues)\n",
      "    extract_set_features('CropinionDataset/reviews_new/test2', 'features/test/classification.txt', aspects, pos_clues, neg_clues)\n",
      "    \n",
      "    \n",
      "    extract_regression_features('CropinionDataset/reviews_new/train2', 'features/train/regression.txt')\n",
      "    extract_regression_features('CropinionDataset/reviews_new/test2', 'features/test/regression.txt')\n",
      "# Prepare both features and true annotations\n",
      "store_feature_vectors()\n",
      "#annotate_sets()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training - Classification\n",
      "\n",
      "A set of grid searches is performed, using k-fold validation (where **k** is 10) to locate the best parameters for precision, recall and F1-score maximisation. The code has been **moved to grid.py**.\n",
      "\n",
      "Currently best parameters when using stems are:\n",
      "\n",
      "- kernel = RBF\n",
      "- C = 1000\n",
      "- Gamma = 0.001\n",
      "\n",
      "While the best for lemmas are:\n",
      "\n",
      "- kernel = RBF\n",
      "- C = 50\n",
      "- Gamma = 0.001\n",
      "\n",
      "Best for lemmas and molex lemas combined:\n",
      "\n",
      "- kernel = RBF\n",
      "- C = 500\n",
      "- Gamma = 0.0001\n",
      "\n",
      "Best for all possible roots (lemmas, stems and molex lemmas) combined (**the one we're using**):\n",
      "\n",
      "- kernel = RBF\n",
      "- C = 500\n",
      "- Gamma = 0.005 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing - Classification\n",
      "\n",
      "This cell performs testing using the developed features and parameters gleaned by cross-validation. The micro and macro scores are computer (P, R and F1) and displayed. All data is scaled.\n",
      "\n",
      "These are the initial results. Improvements using additional features, as well as analysis which features are most important, will be attempted.\n",
      "\n",
      "The recorded results are:\n",
      "\n",
      "## Scores with stems\n",
      "---\n",
      "P = 0.88\n",
      "\n",
      "R = 0.61\n",
      "\n",
      "F1 = 0.72\n",
      "\n",
      "The recall is obviously terrible (stems failed to locate a lot of the pairs).\n",
      "The same experiment must be tried with lemmas, which may improve recall performance greatly.\n",
      "\n",
      "## Scores with lemmas\n",
      "---\n",
      "\n",
      "P = 0.99\n",
      "\n",
      "R = 0.33\n",
      "\n",
      "F1 = 0.50\n",
      "\n",
      "**What the $&@#**\n",
      "\n",
      "## Scores with lemmas AND molex lemmas\n",
      "---\n",
      "\n",
      "P = 0.99\n",
      "\n",
      "R = 0.47\n",
      "\n",
      "F1 = 0.64\n",
      "\n",
      "I have no idea what happened ~ Luka\n",
      "\n",
      "## Scores with all of those combined - *This is what we are using*\n",
      "\n",
      "P = 0.889\n",
      "\n",
      "R = 0.755\n",
      "\n",
      "F1 = 0.816\n",
      "\n",
      "The results achieved with a combination of all three are almost as good as those in the initial paper. Now we only have to improve them by introducing additional features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X_train = np.loadtxt('features/train/classification.txt')\n",
      "y_train = np.loadtxt('features/train/y.txt')[:,0]\n",
      "\n",
      "X_test = np.loadtxt('features/test/classification.txt')\n",
      "y_test = np.loadtxt('features/test/y.txt')[:,0]\n",
      "\n",
      "X_train = scale(X_train)\n",
      "X_test = scale(X_test)\n",
      "\n",
      "N = X_train.shape[0]\n",
      "\n",
      "model = SVC(C=500, gamma=0.0005, kernel='rbf')\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_guess = model.predict(X_test)\n",
      "\n",
      "# ------------- PADDING -------------- #\n",
      "# Some values were probably not located. We must add them to the count.\n",
      "# We do so by padding our vector with 0s and the true vector with 1s\n",
      "\n",
      "cnt = get_dataset_unlocated_pair_count(\n",
      "      'CropinionDataset/reviews_new/test2', \n",
      "      'CropinionDataset/annotated_pairs/all', \n",
      "      aspects, \n",
      "      all_clues)\n",
      "\n",
      "print \"Missed all of\", cnt, \"pairs completely while I at at least tagged\", y_guess.shape[0]\n",
      "\n",
      "y_guess = np.lib.pad(y_guess, (cnt,), 'constant', constant_values=(0,))\n",
      "y_test = np.lib.pad(y_test, (cnt,), 'constant', constant_values=(1,))\n",
      "\n",
      "print \"P  = {0:.3f}\".format(precision_score(y_test, y_guess))\n",
      "print \"R  = {0:.3f}\".format(recall_score(y_test, y_guess))\n",
      "print \"F1 = {0:.3f}\".format(f1_score(y_test, y_guess))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missed all of 23 pairs completely while I at at least tagged 539\n",
        "P  = 0.889\n",
        "R  = 0.755\n",
        "F1 = 0.816\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Score Prediction\n",
      "\n",
      "## Classification - Positive/Negative\n",
      "\n",
      "Predicting whether the scores are positive or negative, where scores of 4 or more are positive and scores of 2.5 or less are negative. Scores inbetween are ambigious and not considered here.\n",
      "\n",
      "The best found parameters when using only BoW + document length are:\n",
      "\n",
      "- kernel = rbf\n",
      "- C = 25\n",
      "- gamma = 0.0005 \n",
      "\n",
      "With those parameters, the positive/negative classification scores are:\n",
      "\n",
      "### Positive\n",
      "\n",
      "\n",
      "F1 = 0.842\n",
      "\n",
      "### Negative\n",
      "\n",
      "F1 = 0.850\n",
      "\n",
      "### Average\n",
      "\n",
      "F1 = 0.846"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X_train = np.loadtxt('features/train/classification.txt')\n",
      "y_train = np.loadtxt('features/train/y.txt')[:,0]\n",
      "\n",
      "X_test = np.loadtxt('features/test/classification.txt')\n",
      "y_test = np.loadtxt('features/test/y.txt')[:,0]\n",
      "\n",
      "X_train = scale(X_train)\n",
      "X_test = scale(X_test)\n",
      "\n",
      "N = X_train.shape[0]\n",
      "\n",
      "model = SVC(C=50, gamma=0.0001, kernel='rbf')\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_guess = model.predict(X_test)\n",
      "\n",
      "p_p = precision_score(y_test, y_guess)\n",
      "r_p = recall_score(y_test, y_guess)\n",
      "f1_p = f1_score(y_test, y_guess)\n",
      "\n",
      "p_n = precision_score(y_test, y_guess, pos_label=0)\n",
      "r_n = recall_score(y_test, y_guess, pos_label=0)\n",
      "f1_n = f1_score(y_test, y_guess, pos_label=0)\n",
      "\n",
      "print \"POSITIVE\\n------------------------\"\n",
      "print \"P  = {0:.3f}\".format(p_p)\n",
      "print \"R  = {0:.3f}\".format(r_p)\n",
      "print \"F1 = {0:.3f}\".format(f1_p)\n",
      "print \"NEGATIVE\\n------------------------\"\n",
      "print \"P  = {0:.3f}\".format(p_n)\n",
      "print \"R  = {0:.3f}\".format(r_n)\n",
      "print \"F1 = {0:.3f}\".format(f1_n)\n",
      "print \"AVERAGE\\n------------------------\"\n",
      "print \"P  = {0:.3f}\".format((p_p + p_n) / 2)\n",
      "print \"R  = {0:.3f}\".format((r_p + r_n) / 2)\n",
      "print \"F1 = {0:.3f}\".format((f1_p + f1_n) / 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "POSITIVE\n",
        "------------------------\n",
        "P  = 0.917\n",
        "R  = 0.778\n",
        "F1 = 0.842\n",
        "NEGATIVE\n",
        "------------------------\n",
        "P  = 0.789\n",
        "R  = 0.922\n",
        "F1 = 0.850\n",
        "AVERAGE\n",
        "------------------------\n",
        "P  = 0.853\n",
        "R  = 0.850\n",
        "F1 = 0.846\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regression\n",
      "\n",
      "Trying to learn the mapping from review to score.\n",
      "\n",
      "We are using Support Vector Regression (SVR) with parameteres tuned via grid search.\n",
      "\n",
      "Best found parameters using only BoW and length are:\n",
      "\n",
      "- kernel = 'rbf'\n",
      "- epsilon = 1\n",
      "- C = 25\n",
      "- gamma = 0.0005\n",
      "\n",
      "The corresponding scores are:\n",
      "\n",
      "MAE = 1.453\n",
      "\n",
      "r = 0.692 (Pearson)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "\n",
      "def pearson(y_true, y_pred):\n",
      "    ret_score = pearsonr(y_true, y_pred)[0]\n",
      "    return ret_score if not np.isnan(ret_score) else 0.0\n",
      "\n",
      "X_train = np.loadtxt('features/train/regression.txt')\n",
      "y_train = np.loadtxt('features/train/z.txt')[:,0]\n",
      "\n",
      "X_test = np.loadtxt('features/test/regression.txt')\n",
      "y_test = np.loadtxt('features/test/z.txt')[:,0]\n",
      "\n",
      "X_train = scale(X_train)\n",
      "X_test = scale(X_test)\n",
      "\n",
      "N = X_train.shape[0]\n",
      "\n",
      "model = SVR(C=25, epsilon = 1, kernel='rbf', gamma=0.0005)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_guess = model.predict(X_test)\n",
      "\n",
      "print 'MAE = {0:.3f}'.format(mean_absolute_error(y_test, y_guess))\n",
      "print 'r = {0:.3f}'.format(pearson(y_test, y_guess))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MAE = 1.453\n",
        "r = 0.692\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helpers\n",
      "\n",
      "\n",
      "Code that is supposed to be run one time or less, like altering the samples, prepping the field and such.\n",
      "\n",
      "**Seriously, don't run this unless you have to**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def store_all_words(directory, word_file):\n",
      "    wf = codecs.open(word_file, 'w', 'utf-8')\n",
      "    \n",
      "    files = files_in(directory)\n",
      "    \n",
      "    i = 1\n",
      "    N = len(files)\n",
      "    \n",
      "    for document in files:\n",
      "        if i % 50 == 0:\n",
      "            print '{}/{}'.format(i, N)\n",
      "        i += 1\n",
      "        \n",
      "        tree = ET.parse(join(directory, document))\n",
      "        root = tree.getroot()\n",
      "        words = root.iter('Word')\n",
      "        \n",
      "        for word in words:\n",
      "            wf.write(word.text + '\\n')\n",
      "            \n",
      "    wf.close()\n",
      "    \n",
      "def write_new_stems(lemma_file, src_dir, dest_dir):\n",
      "    docs = files_in(src_dir)\n",
      "    with codecs.open(lemma_file, 'r', 'utf-8') as lemmas_list:\n",
      "        i = 1\n",
      "        N = len(docs)\n",
      "        \n",
      "        for doc in docs:\n",
      "            if i % 50 == 0:\n",
      "                print '{}/{}'.format(i, N)\n",
      "                \n",
      "            i += 1\n",
      "            \n",
      "            tree = ET.parse(join(src_dir, doc))\n",
      "            root = tree.getroot()\n",
      "            \n",
      "            for word in root.iter('BasicStem'): #'Lemma'\n",
      "                word.text = lemmas_list.readline().split()[1].strip()\n",
      "            \n",
      "            tree.write(join(dest_dir, doc), encoding='utf-8')\n",
      "\n",
      "def print_comment(comment):\n",
      "    text = []\n",
      "\n",
      "    for sentence in comment.get_sentences():\n",
      "        for word in sentence.get_words():\n",
      "            text.append(word.get_lemma())\n",
      "\n",
      "    print '\\n'.join(text)\n",
      "    \n",
      "def print_dependencies(comment):\n",
      "    dependencies = []\n",
      "    \n",
      "    for sentence in comment.get_sentences():\n",
      "        subdep = []\n",
      "        for dependency in sentence.get_dependencies():\n",
      "            subdep.append(str(dependency))\n",
      "        dependencies.append(','.join(subdep))\n",
      "        \n",
      "    print '\\n'.join(dependencies)\n",
      "    \n",
      "#store_all_words('CropinionDataset/reviews_new/train', 'train-words.txt')\n",
      "#write_new_stems('test-lemmas.txt', 'CropinionDataset/reviews_new/test', 'CropinionDataset/reviews_new/test2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50/70\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}