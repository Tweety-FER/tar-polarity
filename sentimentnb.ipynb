{
 "metadata": {
  "name": "",
  "signature": "sha256:8d96305d539a39a3667ab34715e251d8c4bf10b9975983f94c65d9e3ca34b1ec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Aspect-Oriented Sentiment Analysis\n",
      "\n",
      "## Resources\n",
      "\n",
      "[Stemmer](http://nlp.ffzg.hr/resources/tools/stemmer-for-croatian/)\n",
      "\n",
      "[Lemmatiser](http://nlp.ffzg.hr/resources/models/tagging/)\n",
      "\n",
      "S KOJIM BOGOM SU ONI TO LEMATIZIRALI?!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Comment Structure Representation\n",
      "\n",
      "The comments in the dataset are representes as `XML`. To represent it, three classes are used - `TaggedDocument`, `TaggedSentence` and `TaggedWord`.\n",
      "\n",
      "---\n",
      "\n",
      "## `TaggedWord`\n",
      "The class represents a work with associated metadata - index in sentence, lemma, stem, POS tag and so on.\n",
      "\n",
      "## `TaggedSentence`\n",
      "A representation of a sentence with associated metadata. Each sentence has a list of `TaggedWord` objects representing the words that make it up.\n",
      "\n",
      "## `TaggedDocument`\n",
      "A representation of a document and its metadata, with a list of sentences that it contains as a list of `TaggedSentence` objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TaggedText(object):\n",
      "    \"\"\"Object representation of a review document tagged with sentence metadata, source and ratings.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.sentences = []\n",
      "        self.rating = 0\n",
      "        self.source = ''\n",
      "        self.text = ''\n",
      "\n",
      "    def add_sentence(self, sentence):\n",
      "        self.sentences.append(sentence)\n",
      "\n",
      "    def get_sentences(self):\n",
      "        return self.sentences\n",
      "        \n",
      "    def set_filename(self, fn):\n",
      "        self.filename = fn\n",
      "        \n",
      "    def get_filename(self):\n",
      "        return self.filename\n",
      "\n",
      "    def set_source_url(self, url):\n",
      "        self.source = url\n",
      "\n",
      "    def get_source_url(self):\n",
      "        return self.source\n",
      "\n",
      "    def set_rating(self, rating):\n",
      "        self.rating = rating\n",
      "\n",
      "    def get_rating(self):\n",
      "        return self.rating\n",
      "\n",
      "    def set_text(self, text):\n",
      "        self.text = text\n",
      "\n",
      "    def get_text(self):\n",
      "        return self.text\n",
      "\n",
      "class TaggedSentence(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.words = []\n",
      "        self.dependencies = []\n",
      "        self.text = ''\n",
      "        self.start_position = 0\n",
      "        self.length = 0\n",
      "        self.annotated_pairs = []\n",
      "\n",
      "    def add_annotated_pair(self, aspect, anchor, link, sentiment):\n",
      "        r_link = transform_annotation(link)\n",
      "        r_sentiment = transform_annotation(sentiment)\n",
      "        self.annotated_pairs.append((aspect, anchor, r_link, r_sentiment))\n",
      "        \n",
      "    def get_annotated_pairs(self):\n",
      "        return self.annotated_pairs\n",
      "    \n",
      "    def get_annotated_pair(self, aspect, anchor):\n",
      "        aspect_word = aspect.get_word()\n",
      "        anchor_word = anchor.get_word()\n",
      "        \n",
      "        for (r_asp, r_anch, link, sentiment) in self.get_annotated_pairs():\n",
      "            if aspect_word == r_asp and anchor_word == r_anch:\n",
      "                return np.array([link, sentiment])\n",
      "            \n",
      "        return np.array([0, 0])\n",
      "        \n",
      "    def add_word(self, word):\n",
      "        self.words.append(word)\n",
      "\n",
      "    def get_words(self):\n",
      "        return self.words\n",
      "\n",
      "    def add_dependency(self, dependency):\n",
      "        self.dependencies.append(dependency)\n",
      "\n",
      "    def get_dependencies(self):\n",
      "        return self.dependencies\n",
      "\n",
      "    def set_text(self, text):\n",
      "        self.text = text\n",
      "\n",
      "    def get_text(self):\n",
      "        return self.text\n",
      "\n",
      "    def set_start_position(self, pos):\n",
      "        self.start_position = pos\n",
      "\n",
      "    def get_start_position(self):\n",
      "        return self.start_position\n",
      "\n",
      "    def get_length(self):\n",
      "        return len(self.text)\n",
      "    \n",
      "class Dependency(object):\n",
      "    def __init__(self, governor, dependent, relation):\n",
      "        self.governor = governor\n",
      "        self.dependent = dependent\n",
      "        self.relation = relation\n",
      "        \n",
      "    def get_governor_index(self):\n",
      "        return self.governor\n",
      "    \n",
      "    def get_dependent_index(self):\n",
      "        return self.dependent\n",
      "    \n",
      "    def get_relation(self):\n",
      "        return self.relation\n",
      "    \n",
      "    def __str__(self):\n",
      "        return '({0} as {1} to {2})'.format(self.governor, self.relation, self.dependent)\n",
      "\n",
      "\n",
      "class TaggedWord(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.word = ''\n",
      "        self.lemma = ''\n",
      "        self.molex_lemmas = []\n",
      "        self.POS = None\n",
      "        self.stem = ''\n",
      "        self.MSDs = []\n",
      "        self.position = 0\n",
      "        self.index = 0\n",
      "\n",
      "    def set_word(self, word):\n",
      "        self.word = word\n",
      "\n",
      "    def get_word(self):\n",
      "        return self.word\n",
      "\n",
      "    def set_stem(self, stem):\n",
      "        self.stem = stem\n",
      "\n",
      "    def get_stem(self):\n",
      "        return self.stem\n",
      "\n",
      "    def set_lemma(self, lemma):\n",
      "        self.lemma = lemma\n",
      "\n",
      "    def get_lemma(self):\n",
      "        return self.lemma\n",
      "\n",
      "    def add_molex_lemma(self, m_lemma):\n",
      "        self.molex_lemmas.append(m_lemma)\n",
      "\n",
      "    def get_molex_lemmas(self):\n",
      "        return self.molex_lemmas\n",
      "\n",
      "    def set_POS_tag(self, POS):\n",
      "        self.POS = POS\n",
      "\n",
      "    def get_POS_tag(self):\n",
      "        return self.POS\n",
      "\n",
      "    def add_MSD(self, msd):\n",
      "        self.MSDs.append(msd)\n",
      "\n",
      "    def get_MSDs(self):\n",
      "        return self.MSDs\n",
      "\n",
      "    def set_position(self, pos):\n",
      "        self.position = pos\n",
      "\n",
      "    def get_position(self):\n",
      "        return self.position\n",
      "\n",
      "    def set_index(self, index):\n",
      "        self.index = index\n",
      "\n",
      "    def get_index(self):\n",
      "        return self.index\n",
      "    \n",
      "def transform_annotation(annotation):\n",
      "    \"\"\"Transforms string-based orientation annotations into numbers\"\"\"\n",
      "    #if annotation == '+':\n",
      "    #    return 1\n",
      "    #if annotation == '-':\n",
      "    #    return -1\n",
      "    if annotation in \"+-\":\n",
      "        return 1\n",
      "    return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Parsing documents\n",
      "\n",
      "There must be a parsing process that turns the `XML` into the document representation provided above. The function `read_document` does the job for a single document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "import re\n",
      "\n",
      "def files_in(directory):\n",
      "    \"\"\"\n",
      "    List all files in directory. Excludes directories.\n",
      "    Args:\n",
      "        directory (str): String representation of path\n",
      "    Returns:\n",
      "        str[]: List of file names, not prefixed with directory name\n",
      "    \"\"\"\n",
      "    return [f for f in listdir(directory) if isfile(join(directory,f))]\n",
      "\n",
      "def read_documents_in(directory):\n",
      "    \"\"\"\n",
      "    Parses all XML documents in a given directory to create a list of object\n",
      "    document representations.\n",
      "    \n",
      "    Args:\n",
      "        directory (str) : The directory path\n",
      "    \n",
      "    Returns:\n",
      "        TaggedDocument[] : List of documents as object\n",
      "    \"\"\"\n",
      "    files = files_in(directory)\n",
      "    documents = []\n",
      "    \n",
      "    for f in files:\n",
      "        documents.append(read_document(f))\n",
      "        \n",
      "    return documents\n",
      "\n",
      "def read_document(doc):\n",
      "    \"\"\"\n",
      "    Parses an XML document to returns its tagged document representation\n",
      "    Args:\n",
      "        doc (str): The path to the document\n",
      "    Returns:\n",
      "        TaggedDocument: Parsed XML as object\n",
      "    \"\"\"\n",
      "    tree = ET.parse(doc)\n",
      "    root = tree.getroot()\n",
      "    \n",
      "    filename = doc.split('/')[-1].split('.')[0]\n",
      "    \n",
      "    comment = TaggedText()\n",
      "    comment.set_filename(filename) # Name the document\n",
      "    \n",
      "    text = root.find('Text').text\n",
      "    rating = float(root.find('Rating').text)\n",
      "    source = root.find('Source').text\n",
      "\n",
      "    # Setting the basic properties\n",
      "    comment.set_text(text)\n",
      "    comment.set_rating(rating)\n",
      "    comment.set_source_url(source)\n",
      "\n",
      "    # Iterating through all sentences in the text\n",
      "    sentences = root.find('Sentences')\n",
      "\n",
      "    for sentence in sentences.findall('SentenceInfo'):\n",
      "        tagged_sentence = TaggedSentence()\n",
      "\n",
      "        text = sentence.find('Text').text\n",
      "        start = int(sentence.find('StartPosition').text)\n",
      "\n",
      "        tagged_sentence.set_text(text)\n",
      "        tagged_sentence.set_start_position(start)\n",
      "\n",
      "        words = sentence.find('TaggedWords')\n",
      "\n",
      "        for word in words.findall('CROTaggedWord'):\n",
      "            tagged_word = TaggedWord()\n",
      "\n",
      "            word_text = word.find('Word').text\n",
      "            word_lemma = word.find('Lemma').text\n",
      "            molex_lemmas = [molex.text for molex in word.find('MolexLemmas').findall('string')]\n",
      "            POS = word.find('POSTag').text\n",
      "            stem = word.find('BasicStem').text\n",
      "            MSDs = [ msd.text for msd in word.find('MSDs').findall('string')]\n",
      "            position = int(word.find('Position').text)\n",
      "            index = int(word.find('SentenceIndex').text)\n",
      "\n",
      "            tagged_word.set_word(word_text)\n",
      "            tagged_word.set_lemma(word_lemma)\n",
      "            for molex in molex_lemmas: tagged_word.add_molex_lemma(molex)\n",
      "            tagged_word.set_stem(stem)\n",
      "            for msd in MSDs: tagged_word.add_MSD(msd)\n",
      "            tagged_word.set_position(position)\n",
      "            tagged_word.set_index(index)\n",
      "            tagged_word.set_POS_tag(POS)\n",
      "\n",
      "            tagged_sentence.add_word(tagged_word)\n",
      "            \n",
      "        dependencies = sentence.find('DependencyRelations')\n",
      "        \n",
      "        for dependency in dependencies.findall('DependencyRelation'):\n",
      "            governor = int(dependency.find('Governor').find('SentenceIndex').text)\n",
      "            dependent = int(dependency.find('Dependent').find('SentenceIndex').text)\n",
      "            relation = dependency.find('Relation').text\n",
      "            tagged_sentence.add_dependency(Dependency(governor, dependent, relation))\n",
      "            \n",
      "\n",
      "        comment.add_sentence(tagged_sentence)\n",
      "\n",
      "    return comment\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Aspects and Clues\n",
      "\n",
      "For processing to be done, we need to read in the aspects, as well as the positive and negative clues.\n",
      "\n",
      "## tf-idf\n",
      "\n",
      "For viable processing, tf-idf calculations need to be defined"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "import codecs\n",
      "\n",
      "def read_word_stems(document):\n",
      "    \"\"\"\n",
      "        Read stems from a file formatted as\n",
      "        >>>word\\tstem\n",
      "        This format is used for stemmer output\n",
      "    \"\"\"\n",
      "    stems = []\n",
      "    with codecs.open(document, 'r', 'utf-8') as fp:\n",
      "        for line in fp:\n",
      "            stems.append(line.split()[1]) # Stems/lemmas are in the second tab-delimited column\n",
      "    \n",
      "    return set(stems)\n",
      "\n",
      "aspects = read_word_stems('CropinionDataset/dictionary_lemmatised/aspects.txt')\n",
      "pos_clues = read_word_stems('CropinionDataset/dictionary_lemmatised/positive_clues.txt')\n",
      "neg_clues = read_word_stems('CropinionDataset/dictionary_lemmatised/negative_clues.txt')\n",
      "all_clues = pos_clues | neg_clues\n",
      "\n",
      "# We use only aspects and clues as words (at this point)\n",
      "all_words = aspects | all_clues\n",
      "\n",
      "# BROKEN as of yet\n",
      "def calculate_tf_idf(documents, word_list):\n",
      "    tfs = {}\n",
      "    idfs = {}\n",
      "    N = len(documents) # Document count\n",
      "    \n",
      "    for document in documents:\n",
      "        doc_name = document.get_filename()\n",
      "        tfs[doc_name] = {} # Term frequencies for this document\n",
      "        doc_max = 0\n",
      "        \n",
      "        for sentence in document.get_sentences():\n",
      "            for word in sentence.get_words():\n",
      "                lemma = word.get_lemma()\n",
      "                \n",
      "                if lemma in word_list:\n",
      "                    # Adjust local tf\n",
      "                    if lemma in tfs[doc_name]:\n",
      "                        tfs[doc_name][lemma] += 1\n",
      "                    else:\n",
      "                        tfs[doc_name][lemma] = 1\n",
      "                    \n",
      "                    # Adjust document frequency\n",
      "                    if lemma in idfs:\n",
      "                        idfs[lemma][doc_name] = 1\n",
      "                    else:\n",
      "                        idfs[lemma] = { doc_name : 1 }\n",
      "    \n",
      "    # Now we have the counts we need. Let's calculate actual idfs first\n",
      "    for lemma in idfs:\n",
      "        lemma_doc_count = len(idfs[lemma]) + 1.0 #Smooth by 1 to avoid division by zero\n",
      "        idfs[lemma] = log(N / lemma_doc_count)\n",
      "    \n",
      "    # Term frequencies next\n",
      "    for doc_name in tfs:\n",
      "        # Skip documents without any recognised lemmas\n",
      "        if not tfs[doc_name].keys():\n",
      "            continue\n",
      "        max_freq = max(tfs[doc_name].values())\n",
      "        \n",
      "        for lemma in tfs[doc_name]:\n",
      "            tfs[doc_name][lemma] = 0.5 + 0.5 * tfs[doc_name][lemma] / max_freq\n",
      "            \n",
      "    return tfs, idfs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# INCOMPLETE\n",
      "class TfIdfCalculator(object):\n",
      "    def __init__(self, tfs, idfs):\n",
      "        self.tfs = tfs\n",
      "        self.idfs = idfs\n",
      "    \n",
      "    def get_tf_idf(self, lemma, document):\n",
      "        if document not in self.tfs:\n",
      "            return 0\n",
      "        tf = self.tfs[document].get(lemma, 0)\n",
      "        idf = self.idfs.get(lemma, 0)\n",
      "        \n",
      "        return tf * idf\n",
      "    \n",
      "    def set_term_frequencies(self, tfs):\n",
      "        self.tfs = tfs\n",
      "        \n",
      "    def set_inverse_document_frequencies(self, idfs):\n",
      "        self.idfs = idfs\n",
      "    \n",
      "    def get_tf_idf_vector(self, document, all_words):\n",
      "        vector = []\n",
      "        \n",
      "        word_list = sorted(list(all_words)) # Guarantee ordering!\n",
      "        \n",
      "        for word in word_list:\n",
      "            vector.append(self.tf_idf(word, document))\n",
      "        \n",
      "        return np.array(vector)\n",
      "\n",
      "def get_tf_idf_calculator(documents, word_list):\n",
      "    tfs, idfs = calculate_tf_idf(documents, word_list)\n",
      "    return TfIdfCalculator(tfs, idfs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finding potential pairs\n",
      "\n",
      "We need to go through the text and find all potential aspect-clue pairs. Then we tag the bastards."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import remove\n",
      "from os.path import exists, join\n",
      "\n",
      "def find_pairs(sentence, aspects, clues, processor = TaggedWord.get_lemma):\n",
      "    \"\"\"\n",
      "    Finds all potential clue-aspect pairs in the sentence. Pairs are identified by comparing word stems\n",
      "    in the sentence to stems given in the aspect and clue dictionaries (sets).\n",
      "    \n",
      "    Args:\n",
      "        sentence (TaggedSentence) : Object representation of a sentence\n",
      "        aspects (set(str)) : Set of aspect stems\n",
      "        clues (set(str)) : Set of clue stems\n",
      "        \n",
      "    Returns:\n",
      "        ([(TaggedWord, TaggedWord)]) : List of TaggedWord candidate pairs\n",
      "    \"\"\"\n",
      "    sent_aspects = []\n",
      "    sent_clues = []\n",
      "        \n",
      "    for word in sentence.get_words():\n",
      "        root_word = processor(word)\n",
      "        if root_word in aspects:\n",
      "            sent_aspects.append(word)\n",
      "        if root_word in clues:\n",
      "            sent_clues.append(word)\n",
      "        \n",
      "    return [(aspect, clue) for aspect in sent_aspects for clue in sent_clues]\n",
      "\n",
      "def get_annotated_pairs_in_document(document):\n",
      "    tree = ET.parse(document)\n",
      "    root = tree.getroot()\n",
      "\n",
      "    pairs = []\n",
      "    for sentence in root.iter('sentence'):\n",
      "        sent_pairs = []\n",
      "        \n",
      "        for pair in sentence.iter('pair'):\n",
      "            if pair.get('link') in \"+-\":\n",
      "                sent_pairs.append((pair.get('aspect'), pair.get('anchor')))\n",
      "        pairs.append(sent_pairs)\n",
      "            \n",
      "    return pairs\n",
      "\n",
      "def get_unlocated_sentence_pair_count(found_pairs, real_pairs):\n",
      "    unlocated = 0\n",
      "    \n",
      "    for (aspect, clue) in real_pairs:\n",
      "        found = False\n",
      "        for (w_a, w_c) in found_pairs:\n",
      "            w_a, w_c = w_a.get_word(), w_c.get_word()\n",
      "            if (w_a == aspect and w_c == clue) or (w_c == aspect and w_a == clue) :\n",
      "                found = True\n",
      "                break\n",
      "        \n",
      "        if not found:\n",
      "            unlocated += 1\n",
      "            \n",
      "    return unlocated\n",
      "\n",
      "def get_unlocated_document_pair_count(document, ann_document, aspects, clues):\n",
      "    try:\n",
      "        ann_pairs = get_annotated_pairs_in_document(ann_document)\n",
      "    except:\n",
      "        #print ann_document, 'does not exist'\n",
      "        return 0 #No file, missed nothing\n",
      "                \n",
      "    parsed_doc = read_document(document)\n",
      "    \n",
      "    count = 0\n",
      "    \n",
      "    N = len(ann_pairs)\n",
      "    \n",
      "    for i in xrange(N):\n",
      "        my_pairs = find_pairs(parsed_doc.get_sentences()[i], aspects, clues)\n",
      "        ap_pairs = ann_pairs[i]\n",
      "        count += get_unlocated_sentence_pair_count(my_pairs, ap_pairs)\n",
      "        \n",
      "    return count\n",
      " \n",
      "def get_dataset_unlocated_pair_count(dataset_dir, annotated_pairs_dir, aspects, clues):\n",
      "    count = 0\n",
      "    files = files_in(dataset_dir)\n",
      "    \n",
      "    for doc in files:\n",
      "        my_doc = join(dataset_dir, doc)\n",
      "        ap_doc = join(annotated_pairs_dir, doc)\n",
      "        \n",
      "        count += get_unlocated_document_pair_count(my_doc, ap_doc, aspects, clues)\n",
      "        \n",
      "    return count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Extracting actual pairs and their annotations\n",
      "\n",
      "The features are listed and their annotations provided. We must record them in a separate file, for great justice and ease of use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "from os import remove\n",
      "from os.path import exists  \n",
      "\n",
      "def doc_load_annotated_pairs(document, src_dir):\n",
      "    \"\"\"\n",
      "    Given a TaggedDocument representation of an xml document, loads the manually annotated pairs and their\n",
      "    sentiments from a provided source directory. The pair are stored on the document - more precisely, on\n",
      "    each TaggedSentence within the document.\n",
      "    \"\"\"\n",
      "    filename = join(src_dir, document.get_filename() + '.xml')\n",
      "    \n",
      "    if not os.path.isfile(filename):\n",
      "        return False # No pairs\n",
      "        \n",
      "    doc_sentences = document.get_sentences()\n",
      "    \n",
      "    ap_tree = ET.parse(filename)\n",
      "    ap_root = ap_tree.getroot()\n",
      "    \n",
      "    ap_sentences = list(ap_root.iter('sentence'))\n",
      "    \n",
      "    sent_count = len(ap_sentences)\n",
      "    \n",
      "    for i in xrange(sent_count):\n",
      "        ap_sent = ap_sentences[i]\n",
      "        doc_sent = doc_sentences[i]\n",
      "        \n",
      "        # Find all pairs and append them to the doc\n",
      "        for pair in ap_sent.iter('pair'):\n",
      "            doc_sent.add_annotated_pair(pair.get('aspect'), pair.get('anchor'), pair.get('link'), pair.get('sent'))\n",
      "    \n",
      "    return True\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "def doc_annotate_located_pairs(document, aspects, clues):\n",
      "    marks = []\n",
      "    \n",
      "    for sentence in document.get_sentences():\n",
      "        pairs = find_pairs(sentence, aspects, clues)\n",
      "        for pair in pairs:\n",
      "            marks.append(sentence.get_annotated_pair(pair[0], pair[1]))\n",
      "            \n",
      "    \n",
      "    return np.array(marks)\n",
      "\n",
      "def annotate_set_pairs(src_dir, dest_file, annotations_dir, aspects, clues):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    status = False\n",
      "    \n",
      "    for doc in docs:\n",
      "        document = read_document(join(src_dir, doc))\n",
      "        doc_load_annotated_pairs(document, annotations_dir)\n",
      "        np.savetxt(fp, doc_annotate_located_pairs(document, aspects, clues))\n",
      "\n",
      "\n",
      "def annotate_sets():\n",
      "    if(exists('features/train/y.txt')): remove('features/train/y.txt')\n",
      "    if(exists('features/test/y.txt')): remove('features/test/y.txt')  \n",
      "    annotate_set_pairs('CropinionDataset/reviews_new/train2', 'features/train/y.txt', 'CropinionDataset/annotated_pairs/all', aspects, all_clues)\n",
      "    annotate_set_pairs('CropinionDataset/reviews_new/test2', 'features/test/y.txt', 'CropinionDataset/annotated_pairs/all', aspects, all_clues)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Feature extraction\n",
      "\n",
      "Here goes nothing!\n",
      "\n",
      "## Classification features\n",
      "\n",
      "The features used in the initial classification (finding pairs) runs are:\n",
      "\n",
      "- Absolute difference of pair word indices in sentence\n",
      "- Absolute difference of pair word beginnings in string\n",
      "- The length of the sentence\n",
      "- Whether a dependency relation exists between them\n",
      "- Whether their number matches\n",
      "- Whether their gender matches\n",
      "- Whether their cases match\n",
      "- Their POS tags as one-hot vectors\n",
      "- The number of positive and negative clues in the sentence\n",
      "- Whether there is negation in a 3-wide window from **each** word\n",
      "\n",
      "## Regression features\n",
      "\n",
      "The features (to be) used in the initial regression run:\n",
      "\n",
      "- tf-idf (still somewhat broken)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parsed_docs = []\n",
      "files = files_in('CropinionDataset/reviews_new/train')\n",
      "\n",
      "for doc in files:\n",
      "    fname = join('CropinionDataset/reviews_new/train', doc)\n",
      "    parsed_docs.append(read_document(fname))\n",
      "\n",
      "tfidf = get_tf_idf_calculator(parsed_docs, all_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_set_features(src_dir, dest_file, aspects, pos_clues, neg_clues):\n",
      "    docs = files_in(src_dir)\n",
      "    fp = codecs.open(dest_file, 'a', 'utf-8')\n",
      "    \n",
      "    for doc in docs:\n",
      "        features = extract_document_features(join(src_dir, doc), aspects, pos_clues, neg_clues)\n",
      "        \n",
      "        np.savetxt(fp, features)\n",
      "    \n",
      "    fp.close()\n",
      "\n",
      "def extract_document_features(document_path, aspects, pos_clues, neg_clues):\n",
      "    document = read_document(document_path)\n",
      "    clues = pos_clues | neg_clues\n",
      "    \n",
      "    features = []\n",
      "    for sentence in document.get_sentences():\n",
      "        pairs = find_pairs(sentence, aspects, clues)\n",
      "        for pair in pairs:\n",
      "            features.append(compute_feature_vector(pair, sentence, pos_clues, neg_clues))\n",
      "            \n",
      "    return np.array(features)\n",
      "\n",
      "def compute_feature_vector(pair, sentence, pos_clues, neg_clues):\n",
      "    features = []\n",
      "    \n",
      "    features.append(pair_distance_index(pair))\n",
      "    features.append(pair_init_distances(pair))\n",
      "    features.append(sentence_length(sentence))\n",
      "    features.append(govern_relations_exist(pair, sentence))\n",
      "    features.append(match(pair, extract_plurality))\n",
      "    features.append(match(pair, extract_genders))\n",
      "    features.append(POS_vector(pair[0]))\n",
      "    features.append(POS_vector(pair[1]))\n",
      "    features.append(aspect_counts(sentence, pos_clues, neg_clues))\n",
      "    features.append(negation_present(pair[0], sentence))\n",
      "    features.append(negation_present(pair[1], sentence))\n",
      "    \n",
      "    return np.hstack(features)\n",
      "\n",
      "def aspect_counts(sentence, pos_clues, neg_clues, processor = TaggedWord.get_lemma):\n",
      "    counts = [0, 0]\n",
      "    for word in sentence.get_words():\n",
      "        root = processor(word)\n",
      "        if root in pos_clues:\n",
      "            counts[0] += 1\n",
      "        if root in neg_clues:\n",
      "            counts[1] += 1\n",
      "    \n",
      "    return np.array(counts)\n",
      "        \n",
      "def pair_distance_index(pair):\n",
      "    return np.array([abs(pair[0].get_index() - pair[1].get_index())])\n",
      "\n",
      "def pair_init_distances(pair):\n",
      "    return np.array([abs(pair[0].get_position() - pair[1].get_position())])\n",
      "\n",
      "def sentence_length(sentence):\n",
      "    return np.array([sentence.get_length()])\n",
      "\n",
      "def govern_relations_exist(pair, sentence):\n",
      "    rels = [0, 0] # Aspect governs clue, clue governs aspect\n",
      "    index_pair = pair[0].get_index(), pair[1].get_index()\n",
      "    \n",
      "    for dependency in sentence.get_dependencies():\n",
      "        (a, b) = dependency.get_governor_index(), dependency.get_dependent_index()\n",
      "        if (a,b) == index_pair:\n",
      "            rels[0] = 1\n",
      "        elif (b, a) == index_pair:\n",
      "            rels[1] = 1\n",
      "    \n",
      "    return np.array(rels)\n",
      "\n",
      "def match(pair, f):\n",
      "    options_a = f(pair[0])\n",
      "    options_b = f(pair[1])\n",
      "    \n",
      "    for a_g in options_a:\n",
      "        for b_g in options_b:\n",
      "            if a_g == b_g:\n",
      "                return np.array([1.0])\n",
      "    return np.array([0.0])\n",
      "\n",
      "def POS_vector(word):\n",
      "    tags = \"ACIMNPQRSVYZ\" # POS tags in set\n",
      "    one_hot = np.zeros(12)\n",
      "    \n",
      "    #print tags, word.get_POS_tag()\n",
      "    index = tags.index(word.get_POS_tag())\n",
      "    \n",
      "    if index != -1:\n",
      "        one_hot[index] = 1\n",
      "        \n",
      "    return one_hot\n",
      "\n",
      "def extract_plurality(word):\n",
      "    pluralities = set()\n",
      "    \n",
      "    msds = word.get_MSDs()\n",
      "    for msd in msds:\n",
      "        if msd[0] == 'A':\n",
      "            pluralities.add(msd[4])\n",
      "        elif msd[0] == 'N':\n",
      "            pluralities.add(msd[3])\n",
      "        elif msd[0] == 'V':\n",
      "            pluralities.add(msds[-1])\n",
      "            \n",
      "    return list(pluralities)\n",
      "\n",
      "def extract_genders(word):\n",
      "    genders = set()\n",
      "    \n",
      "    msds = word.get_MSDs()\n",
      "    for msd in msds:\n",
      "        if msd[0] == 'A':\n",
      "            genders.add(msd[3])\n",
      "        elif msd[0] == 'N':\n",
      "            genders.add(msd[2])\n",
      "            \n",
      "    return list(genders)\n",
      "\n",
      "def negation_present(word, sentence, k = 3):\n",
      "    negations = [u'ne', u'nije', u'nimalo', u'nipo\u0161to', u'nisam', u'nisu', u'nismo', u'nemojte']\n",
      "    word_index = word.get_index()\n",
      "    negation_present = 0\n",
      "    \n",
      "    words = sentence.get_words()\n",
      "    N = len(words)\n",
      "    \n",
      "    start = max(0, word_index - k)\n",
      "    end   = min(N - 1, word_index + k)\n",
      "    \n",
      "    while start <= end:\n",
      "        if start == word_index:\n",
      "            start += 1\n",
      "            continue\n",
      "        \n",
      "        if words[start].get_word().lower() in negations:\n",
      "            negation_present = 1\n",
      "            break # Just looking for presence, not count\n",
      "            \n",
      "        start += 1\n",
      "    \n",
      "    return np.array([negation_present])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import remove\n",
      "from os.path import exists\n",
      "\n",
      "def store_feature_vectors():\n",
      "    \"\"\"\n",
      "    Calculate the feature vectors for the entire train and test sets and store the resulting numpy vectors\n",
      "    into files for easy access. Should be run every time the way features are calculated has changed.\n",
      "    \"\"\"\n",
      "    #Remove old ones\n",
      "    if(exists('features/train/nonscaled.txt')): remove('features/train/nonscaled.txt')\n",
      "    if(exists('features/test/nonscaled.txt')): remove('features/test/nonscaled.txt')    \n",
      "        \n",
      "    extract_set_features('CropinionDataset/reviews_new/train2', 'features/train/nonscaled.txt', aspects, pos_clues, neg_clues)\n",
      "    extract_set_features('CropinionDataset/reviews_new/test2', 'features/test/nonscaled.txt', aspects, pos_clues, neg_clues)\n",
      "    \n",
      "# Prepare both features and true annotations\n",
      "store_feature_vectors()\n",
      "annotate_sets()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training - Classification\n",
      "\n",
      "This cell can be run independently of all others, provided you have the required dataset manipulations saved to disk.\n",
      "The required elements are:\n",
      "\n",
      "- Parsed and saved features\n",
      "- Parsed and saved target classes\n",
      "\n",
      "A set of grid searcher is performed, using k-fold validation (where **k** is 10) to locate the best parameters for precision, recall and F1-score maximisation. **DO NOT RUN UNLESS YOU HAVE TO**, takes forever (a few hours).\n",
      "\n",
      "Currently best parameters when using stems are:\n",
      "\n",
      "- kernel RBF\n",
      "- C = 1000\n",
      "- Gamma = 0.001\n",
      "\n",
      "While the best for lemmas are:\n",
      "\n",
      "- kernel RBF\n",
      "- C = 50\n",
      "- Gamma = 0.001"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report, make_scorer\n",
      "\n",
      "#Number of folds\n",
      "k = 10\n",
      "\n",
      "X_train = np.loadtxt('features/train/nonscaled.txt')\n",
      "y_train = np.loadtxt('features/train/y.txt')[:,0]\n",
      "\n",
      "X_test = np.loadtxt('features/test/nonscaled.txt')\n",
      "y_test = np.loadtxt('features/test/y.txt')[:,0]\n",
      "\n",
      "X_train = scale(X_train)\n",
      "X_test = scale(X_test)\n",
      "N = X_train.shape[0]\n",
      "#kf = KFold(N, n_folds = k) # k-fold validation subsets\n",
      "\n",
      "\n",
      "# ---------- GRID SETUP ----------- #\n",
      "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4],\n",
      "                     'C': [1, 10, 100, 1000]},\n",
      "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
      "\n",
      "scores = [('f1',make_scorer(f1_score))]\n",
      "\n",
      "for score in scores:\n",
      "    print(\"# Tuning hyper-parameters for %s\" % score[0])\n",
      "    print()\n",
      "\n",
      "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=k,\n",
      "                       scoring=score[1])\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    print(\"Best parameters set found on development set:\")\n",
      "    print()\n",
      "    print(clf.best_params_)\n",
      "    print()\n",
      "    print(\"Grid scores on development set:\")\n",
      "    print()\n",
      "    for params, mean_score, scores in clf.grid_scores_:\n",
      "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "              % (mean_score, scores.std() * 2, params))\n",
      "    print()\n",
      "\n",
      "    print(\"Detailed classification report:\")\n",
      "    print()\n",
      "    print(\"The model is trained on the full development set.\")\n",
      "    print(\"The scores are computed on the full evaluation set.\")\n",
      "    print()\n",
      "    y_true, y_pred = y_test, clf.predict(X_test)\n",
      "    print(classification_report(y_true, y_pred))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Testing - Classification\n",
      "\n",
      "This cell performs testing using the developed features and parameters gleaned by cross-validation. The micro and macro scores are computer (P, R and F1) and displayed. All data is scaled.\n",
      "\n",
      "These are the initial results. Improvements using additional features, as well as analysis which features are most important, will be attempted.\n",
      "\n",
      "The recorded results are:\n",
      "\n",
      "## Scores with stems\n",
      "---\n",
      "P = 0.88\n",
      "\n",
      "R = 0.61\n",
      "\n",
      "F1 = 0.72\n",
      "\n",
      "The recall is obviously terrible (stems failed to locate a lot of the pairs).\n",
      "The same experiment must be tried with lemmas, which may improve recall performance greatly.\n",
      "\n",
      "## Scores with lemmas\n",
      "---\n",
      "\n",
      "P = 0.99\n",
      "\n",
      "R = 0.33\n",
      "\n",
      "F1 = 0.50\n",
      "\n",
      "**What the $&@#**\n",
      "\n",
      "I have no idea what happened ~ Luka"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X_train = np.loadtxt('features/train/nonscaled.txt')\n",
      "y_train = np.loadtxt('features/train/y.txt')[:,0]\n",
      "\n",
      "X_test = np.loadtxt('features/test/nonscaled.txt')\n",
      "y_test = np.loadtxt('features/test/y.txt')[:,0]\n",
      "\n",
      "X_train = scale(X_train)\n",
      "X_test = scale(X_test)\n",
      "\n",
      "N = X_train.shape[0]\n",
      "\n",
      "model = SVC(C=50, gamma=0.001, kernel='rbf')\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_guess = model.predict(X_test)\n",
      "\n",
      "# ------------- PADDING -------------- #\n",
      "# Some values were probably not located. We must add them to the count.\n",
      "# We do so by padding our vector with 0s and the true vector with 1s\n",
      "\n",
      "cnt = get_dataset_unlocated_pair_count(\n",
      "      'CropinionDataset/reviews_new/test2', \n",
      "      'CropinionDataset/annotated_pairs/all', \n",
      "      aspects, \n",
      "      all_clues)\n",
      "\n",
      "print \"Missed all of\", cnt, \"pairs completely while I at at least tagged\", y_guess.shape[0]\n",
      "\n",
      "y_guess = np.lib.pad(y_guess, (cnt,), 'constant', constant_values=(0,))\n",
      "y_test = np.lib.pad(y_test, (cnt,), 'constant', constant_values=(1,))\n",
      "\n",
      "print \"P  = {0:.2f}\".format(precision_score(y_test, y_guess))\n",
      "print \"R  = {0:.2f}\".format(recall_score(y_test, y_guess))\n",
      "print \"F1 = {0:.2f}\".format(f1_score(y_test, y_guess))\n",
      "\n",
      "#print \"Guess\\tReal\"\n",
      "#for i in xrange(y_guess.shape[0]):\n",
      "#    print \"{}\\t{}\".format(y_guess[i], y_test[i]),\n",
      "#    print \"\" if y_guess[i] >= y_test[i] else \"<--- HERE!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Missed all of 135 pairs completely while I at at least tagged 276\n",
        "P  = 0.99\n",
        "R  = 0.33\n",
        "F1 = 0.50\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helpers\n",
      "\n",
      "\n",
      "Code that is supposed to be run one time or less, like altering the samples, prepping the field and such.\n",
      "\n",
      "**Seriously, don't run this unless you have to**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def store_all_words(directory, word_file):\n",
      "    wf = codecs.open(word_file, 'w', 'utf-8')\n",
      "    \n",
      "    files = files_in(directory)\n",
      "    \n",
      "    i = 1\n",
      "    N = len(files)\n",
      "    \n",
      "    for document in files:\n",
      "        if i % 50 == 0:\n",
      "            print '{}/{}'.format(i, N)\n",
      "        i += 1\n",
      "        \n",
      "        tree = ET.parse(join(directory, document))\n",
      "        root = tree.getroot()\n",
      "        words = root.iter('Word')\n",
      "        \n",
      "        for word in words:\n",
      "            wf.write(word.text + '\\n')\n",
      "            \n",
      "    wf.close()\n",
      "    \n",
      "def write_new_stems(lemma_file, src_dir, dest_dir):\n",
      "    docs = files_in(src_dir)\n",
      "    with codecs.open(lemma_file, 'r', 'utf-8') as lemmas_list:\n",
      "        i = 1\n",
      "        N = len(docs)\n",
      "        \n",
      "        for doc in docs:\n",
      "            if i % 50 == 0:\n",
      "                print '{}/{}'.format(i, N)\n",
      "                \n",
      "            i += 1\n",
      "            \n",
      "            tree = ET.parse(join(src_dir, doc))\n",
      "            root = tree.getroot()\n",
      "            \n",
      "            for word in root.iter('BasicStem'): #'Lemma'\n",
      "                word.text = lemmas_list.readline().split()[1].strip()\n",
      "            \n",
      "            tree.write(join(dest_dir, doc), encoding='utf-8')\n",
      "\n",
      "def print_comment(comment):\n",
      "    text = []\n",
      "\n",
      "    for sentence in comment.get_sentences():\n",
      "        for word in sentence.get_words():\n",
      "            text.append(word.get_lemma())\n",
      "\n",
      "    print '\\n'.join(text)\n",
      "    \n",
      "def print_dependencies(comment):\n",
      "    dependencies = []\n",
      "    \n",
      "    for sentence in comment.get_sentences():\n",
      "        subdep = []\n",
      "        for dependency in sentence.get_dependencies():\n",
      "            subdep.append(str(dependency))\n",
      "        dependencies.append(','.join(subdep))\n",
      "        \n",
      "    print '\\n'.join(dependencies)\n",
      "    \n",
      "#store_all_words('CropinionDataset/reviews_new/train', 'train-words.txt')\n",
      "#write_new_stems('test-lemmas.txt', 'CropinionDataset/reviews_new/test', 'CropinionDataset/reviews_new/test2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50/70\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}